{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import cdist\n",
    "import scipy.stats as stats\n",
    "import multiprocessing\n",
    "\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_folder(folder_path):\n",
    "    files = os.listdir(folder_path)\n",
    "    \n",
    "    df_list = []\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df_list.append(df)\n",
    "    \n",
    "    combined_df = df_list[0]\n",
    "    \n",
    "    for df in df_list[1:]:\n",
    "        combined_df = pd.merge(combined_df, df, on='id', how='outer')\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "test_folder = '../data/test/'\n",
    "test_df = load_data_from_folder(test_folder)\n",
    "\n",
    "train_folder = '../data/train/'\n",
    "train_df = load_data_from_folder(train_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_df, test_size=0.3, random_state=42):\n",
    "    X = train_df.drop(['attack_cat', 'label'], axis=1) \n",
    "    y = train_df['attack_cat'] \n",
    "    \n",
    "    numeric_columns = X.select_dtypes(include=['number']).columns\n",
    "    categorical_columns = X.select_dtypes(exclude=['number']).columns\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        X[col] = X[col].fillna(X[col].median())\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        X[col] = X[col].fillna(X[col].mode()[0])\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X[numeric_columns] = scaler.fit_transform(X[numeric_columns])\n",
    "    \n",
    "    label_encoders = {}\n",
    "    for col in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col])\n",
    "        label_encoders[col] = le\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    smote = SMOTE(random_state=random_state)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    return X_train_resampled, X_test, y_train_resampled, y_test, scaler, label_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test_data(test_df, scaler, label_encoders):\n",
    "    X_test = test_df.copy()\n",
    "    \n",
    "    numeric_columns = X_test.select_dtypes(include=['number']).columns\n",
    "    categorical_columns = X_test.select_dtypes(exclude=['number']).columns\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        X_test[col] = X_test[col].fillna(X_test[col].median())\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        X_test[col] = X_test[col].fillna(X_test[col].mode()[0])\n",
    "\n",
    "    X_test[numeric_columns] = scaler.transform(X_test[numeric_columns])\n",
    "\n",
    "    for col in categorical_columns:\n",
    "        if col in label_encoders:\n",
    "            le = label_encoders[col]\n",
    "            if 'unknown' not in le.classes_:\n",
    "                le.classes_ = np.append(le.classes_, 'unknown')\n",
    "            \n",
    "            X_test[col] = X_test[col].apply(lambda x: x if x in le.classes_ else 'unknown')\n",
    "            X_test[col] = le.transform(X_test[col])\n",
    "    \n",
    "    return X_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedKNNClassifier:\n",
    "    def __init__(self, k=3, metric=\"euclidean\", weights=\"distance\", \n",
    "                 batch_size=1000, n_jobs=-1, algorithm='auto'):\n",
    "        \"\"\"\n",
    "        Enhanced Memory-Efficient KNN Classifier with multiple optimizations.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        k : int, default=3\n",
    "            Number of neighbors to use\n",
    "        metric : str, default=\"euclidean\"\n",
    "            Distance metric to use ('euclidean', 'manhattan', 'minkowski')\n",
    "        weights : str, default=\"distance\"\n",
    "            Weight function used in prediction ('uniform' or 'distance')\n",
    "        batch_size : int, default=1000\n",
    "            Number of test points to process in each batch\n",
    "        n_jobs : int, default=-1\n",
    "            Number of parallel jobs (-1 uses all available cores)\n",
    "        algorithm : str, default='auto'\n",
    "            Algorithm for nearest neighbor search ('auto', 'brute', 'kd_tree', 'ball_tree')\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.metric = metric\n",
    "        self.weights = weights\n",
    "        self.batch_size = batch_size\n",
    "        self.n_jobs = n_jobs if n_jobs != -1 else multiprocessing.cpu_count()\n",
    "        self.algorithm = algorithm\n",
    "        \n",
    "        self.train_data = None\n",
    "        self.train_labels = None\n",
    "        self._kdtree = None\n",
    "        self._balltree = None\n",
    "    \n",
    "    def fit(self, train_data, train_labels):\n",
    "\n",
    "        self.train_data = np.asarray(train_data, dtype=np.float32, order='C')\n",
    "        self.train_labels = np.asarray(train_labels)\n",
    "\n",
    "        if self.algorithm in ['kd_tree', 'auto']:\n",
    "            try:\n",
    "                from sklearn.neighbors import KDTree\n",
    "                self._kdtree = KDTree(self.train_data, metric=self.metric)\n",
    "            except ImportError:\n",
    "                self.algorithm = 'brute'\n",
    "        \n",
    "        if self.algorithm in ['ball_tree', 'auto']:\n",
    "            try:\n",
    "                from sklearn.neighbors import BallTree\n",
    "                self._balltree = BallTree(self.train_data, metric=self.metric)\n",
    "            except ImportError:\n",
    "                self.algorithm = 'brute'\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _parallel_distance_computation(self, test_batch):\n",
    "        \"\"\"\n",
    "        Compute distances using parallel processing.\n",
    "        \"\"\"\n",
    "        if self.algorithm == 'kd_tree' and self._kdtree:\n",
    "            distances, indices = self._kdtree.query(test_batch, k=self.k)\n",
    "            return distances, indices\n",
    "        \n",
    "        if self.algorithm == 'ball_tree' and self._balltree:\n",
    "            distances, indices = self._balltree.query(test_batch, k=self.k)\n",
    "            return distances, indices\n",
    "        \n",
    "        # Fallback to manual distance computation\n",
    "        if self.metric == 'euclidean':\n",
    "            distances = cdist(test_batch, self.train_data, metric='euclidean')\n",
    "        elif self.metric == 'manhattan':\n",
    "            distances = cdist(test_batch, self.train_data, metric='cityblock')\n",
    "        elif self.metric == 'minkowski':\n",
    "            distances = cdist(test_batch, self.train_data, metric='minkowski')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric: {self.metric}\")\n",
    "        \n",
    "        # Get indices of k nearest neighbors for each point\n",
    "        k_indices = np.argsort(distances, axis=1)[:, :self.k]\n",
    "        k_distances = np.take_along_axis(distances, k_indices, axis=1)\n",
    "        \n",
    "        return k_distances, k_indices\n",
    "    \n",
    "    def _weighted_voting(self, k_labels, k_distances):\n",
    "        \"\"\"\n",
    "        Advanced voting mechanism with sophisticated weight calculation.\n",
    "        \"\"\"\n",
    "        if self.weights == 'uniform':\n",
    "            # Simple majority voting\n",
    "            return stats.mode(k_labels)[0][0]\n",
    "        \n",
    "        elif self.weights == 'distance':\n",
    "            # Weighted voting with inverse distance\n",
    "            inv_distances = 1.0 / (k_distances + 1e-8)\n",
    "            normalized_weights = inv_distances / np.sum(inv_distances)\n",
    "            \n",
    "            # Compute weighted votes\n",
    "            unique_labels = np.unique(k_labels)\n",
    "            weighted_votes = []\n",
    "            for label in unique_labels:\n",
    "                label_mask = (k_labels == label)\n",
    "                weighted_vote = np.sum(normalized_weights[label_mask])\n",
    "                weighted_votes.append((label, weighted_vote))\n",
    "            \n",
    "            return max(weighted_votes, key=lambda x: x[1])[0]\n",
    "    \n",
    "    def predict(self, test_points):\n",
    "        \"\"\"\n",
    "        Predict labels for test points with enhanced efficiency.\n",
    "        \"\"\"\n",
    "        # Validate training data\n",
    "        if self.train_data is None:\n",
    "            raise ValueError(\"Model has not been trained. Call 'fit' first.\")\n",
    "        \n",
    "        # Prepare test points\n",
    "        test_points = np.asarray(test_points, dtype=np.float32)\n",
    "        predictions = []\n",
    "        \n",
    "        # Process in batches with potential parallel processing\n",
    "        for i in range(0, len(test_points), self.batch_size):\n",
    "            test_batch = test_points[i:i+self.batch_size]\n",
    "            \n",
    "            # Compute distances and nearest neighbor indices\n",
    "            k_distances, k_indices = self._parallel_distance_computation(test_batch)\n",
    "            \n",
    "            # Predict for each point in the batch\n",
    "            batch_predictions = []\n",
    "            for distances, indices in zip(k_distances, k_indices):\n",
    "                # Get labels of k nearest neighbors\n",
    "                k_labels = self.train_labels[indices]\n",
    "                \n",
    "                # Apply advanced voting\n",
    "                prediction = self._weighted_voting(k_labels, distances)\n",
    "                batch_predictions.append(prediction)\n",
    "            \n",
    "            predictions.extend(batch_predictions)\n",
    "        \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def score(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Compute the accuracy of the classifier.\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X_test)\n",
    "        return np.mean(predictions == y_test)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (f\"MemoryEfficientKNNClassifier(k={self.k}, \"\n",
    "                f\"metric='{self.metric}', \"\n",
    "                f\"weights='{self.weights}', \"\n",
    "                f\"batch_size={self.batch_size})\")\n",
    "        \n",
    "    def submit(self, X_test, filename='submission.csv'):\n",
    "        \"\"\"\n",
    "        Make predictions on test data and save them to a CSV file.\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X_test)\n",
    "        submission_df = pd.DataFrame({'id': test_df['id'], 'attack_cat': predictions})\n",
    "        submission_df.to_csv(filename, index=False)\n",
    "        return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names must be in the same order as they were in fit.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m X_train, X_test, y_train, y_test, scaler, label_encoders \u001b[38;5;241m=\u001b[39m preprocess_data(train_df)\n\u001b[0;32m----> 2\u001b[0m X_test \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_test_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_encoders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m knn \u001b[38;5;241m=\u001b[39m ImprovedKNNClassifier(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m'\u001b[39m, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, algorithm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m knn\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m, in \u001b[0;36mpreprocess_test_data\u001b[0;34m(test_df, scaler, label_encoders)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m categorical_columns:\n\u001b[1;32m     11\u001b[0m     X_test[col] \u001b[38;5;241m=\u001b[39m X_test[col]\u001b[38;5;241m.\u001b[39mfillna(X_test[col]\u001b[38;5;241m.\u001b[39mmode()[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 13\u001b[0m X_test[numeric_columns] \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnumeric_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m categorical_columns:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m label_encoders:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/sklearn/preprocessing/_data.py:1043\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1040\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1042\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m-> 1043\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/sklearn/base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[1;32m    545\u001b[0m ):\n\u001b[1;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    614\u001b[0m         )\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/sklearn/base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m     )\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names must be in the same order as they were in fit.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, scaler, label_encoders = preprocess_data(train_df)\n",
    "X_test = preprocess_test_data(test_df, scaler, label_encoders)\n",
    "\n",
    "knn = ImprovedKNNClassifier(k=5, metric='euclidean', weights='distance', batch_size=1000, n_jobs=-1, algorithm='auto')\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "knn.submit(X_test, filename='submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
